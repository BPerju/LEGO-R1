{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e682ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_folder = snapshot_download(\n",
    "    repo_id=\"PPPPPeter/arta\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"ARTA_LEGO\"\n",
    ")\n",
    "\n",
    "print(\"All files are in:\", local_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('smoothness')\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "file_path = 'lego_dataset_cot_grnd.jsonl' \n",
    "\n",
    "f1_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "bleu_scores = []\n",
    "\n",
    "def extract_answer_content(text):\n",
    "    \"\"\"\n",
    "    Extracts the content inside the first <answer>...</answer> tag pair.\n",
    "    Returns the content string or the original text if tags are not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    match = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else text.strip()\n",
    "\n",
    "def compute_word_level_f1(pred_words, label_words):\n",
    "    # Create word count dictionaries\n",
    "    pred_count = Counter(pred_words)\n",
    "    label_count = Counter(label_words)\n",
    "    \n",
    "    all_words = set(pred_count.keys()).union(set(label_count.keys()))\n",
    "    \n",
    "    y_pred = [1 if word in pred_count else 0 for word in all_words]\n",
    "    y_true = [1 if word in label_count else 0 for word in all_words]\n",
    "    \n",
    "    return f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        prediction_full = data.get(\"predict\", \"\").strip()\n",
    "        label_full = data.get(\"label\", \"\").strip()\n",
    "        \n",
    "        # Extract content from <answer> tags\n",
    "        prediction = extract_answer_content(prediction_full)\n",
    "        label = extract_answer_content(label_full)\n",
    "        \n",
    "        # Tokenize predictions and labels\n",
    "        pred_words = word_tokenize(prediction.lower()) if prediction else []\n",
    "        label_words = word_tokenize(label.lower()) if label else []\n",
    "\n",
    "        # Compute BLEU score (using smoothing to avoid zero scores for short sequences)\n",
    "        smoothie = SmoothingFunction().method1\n",
    "        if len(label_words) > 0:\n",
    "            bleu = sentence_bleu([label_words], pred_words, smoothing_function=smoothie)\n",
    "        else:\n",
    "            bleu = 0.0  # No reference words\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "        # Compute word-level F1\n",
    "        f1 = compute_word_level_f1(pred_words, label_words)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        rouge_scores = scorer.score(label, prediction)\n",
    "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "print(f\"Average Word-Level F1: {sum(f1_scores)/len(f1_scores):.4f}\" if f1_scores else \"N/A\")\n",
    "print(f\"Average ROUGE-1 F1: {sum(rouge1_scores)/len(rouge1_scores):.4f}\" if rouge1_scores else \"N/A\")\n",
    "print(f\"Average ROUGE-2 F1: {sum(rouge2_scores)/len(rouge2_scores):.4f}\" if rouge2_scores else \"N/A\")\n",
    "print(f\"Average ROUGE-L F1: {sum(rougeL_scores)/len(rougeL_scores):.4f}\" if rougeL_scores else \"N/A\")\n",
    "print(f\"Average BLEU Score: {sum(bleu_scores)/len(bleu_scores):.4f}\" if bleu_scores else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feee298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "file_path = 'lego_dataset_cot_state.jsonl' \n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "def extract_answer_content(text):\n",
    "    \"\"\"\n",
    "    Extracts the content inside the first <answer>...</answer> tag pair.\n",
    "    Returns the content string or the original text if tags are not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    match = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else text.strip()\n",
    "\n",
    "def parse_response(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "    # Extract first occurrence of Yes/No (case-insensitive)\n",
    "    match = re.search(r'\\b(yes|no)\\b', text.strip().lower())\n",
    "    if match:\n",
    "        return match.group(1).capitalize() \n",
    "    return None \n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        data = json.loads(line.strip())\n",
    "        predict_full = data.get(\"predict\", \"\")\n",
    "        label_full = data.get(\"label\", \"\")\n",
    "\n",
    "        # Extract content from <answer> tags\n",
    "        predict_text = extract_answer_content(predict_full)\n",
    "        label_text = extract_answer_content(label_full)\n",
    "\n",
    "\n",
    "        pred_parsed = parse_response(predict_text)\n",
    "        label_parsed = parse_response(label_text)\n",
    "\n",
    "        if pred_parsed is None or label_parsed is None:\n",
    "            print(f\"Skipping line {line_num} due to invalid format (pred={pred_parsed}, label={label_parsed})\")\n",
    "            continue\n",
    "\n",
    "        y_pred.append(pred_parsed)\n",
    "        y_true.append(label_parsed)\n",
    "\n",
    "# Convert to binary: Yes=1, No=0 for sklearn metrics\n",
    "y_true_bin = [1 if y == \"Yes\" else 0 for y in y_true]\n",
    "y_pred_bin = [1 if y == \"Yes\" else 0 for y in y_pred]\n",
    "\n",
    "f1_state = f1_score(y_true_bin, y_pred_bin, pos_label=1)  \n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin).ravel()\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "print(f\"Total Examples: {len(y_true)}\")\n",
    "print(f\"F1-State (F1 on 'Yes'): {f1_state:.4f}\")\n",
    "print(f\"False Positive Rate (FPR): {fpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01581dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def extract_answer_content(text):\n",
    "    \"\"\"\n",
    "    Extracts the content inside the first <answer>...</answer> tag pair.\n",
    "    Returns the content string or the original text if tags are not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    match = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else text.strip()\n",
    "\n",
    "\n",
    "def compute_word_level_f1(gt_labels, pred_labels):\n",
    "    \"\"\"Compute token-level F1 between all labels (case-insensitive)\"\"\"\n",
    "    if not gt_labels or not pred_labels:\n",
    "        return 0.0\n",
    "\n",
    "    gt_text = \" \".join(gt_labels).lower()\n",
    "    pred_text = \" \".join(pred_labels).lower()\n",
    "\n",
    "    gt_tokens = word_tokenize(gt_text)\n",
    "    pred_tokens = word_tokenize(pred_text)\n",
    "\n",
    "    common = set(gt_tokens) & set(pred_tokens)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return f1\n",
    "\n",
    "\n",
    "def parse_json_boxes_and_text(json_str):\n",
    "    \"\"\"\n",
    "    Parse a string that should be a list of dicts:\n",
    "    [{'bbox_2d': [x1,y1,x2,y2], 'label': '...'}]\n",
    "    \n",
    "    Handles single quotes, malformed JSON, missing keys.\n",
    "    \"\"\"\n",
    "    if not isinstance(json_str, str) or not json_str:\n",
    "        return [], []\n",
    "\n",
    "    # Clean up\n",
    "    json_str = json_str.strip()\n",
    "    json_str = json_str.replace(\"'\", '\"')  \n",
    "    json_str = re.sub(r'\\b(\\d+)\\s*([a-zA-Z])', r'\\1 \\2', json_str) \n",
    "    json_str = re.sub(r'\\b([a-zA-Z])\\s*(\\d+)\\b', r'\\1 \\2', json_str) \n",
    "\n",
    "    if not json_str.startswith('['):\n",
    "        json_str = '[' + json_str\n",
    "    if not json_str.endswith(']'):\n",
    "        json_str = json_str + ']'\n",
    "\n",
    "    try:\n",
    "        boxes_data = json.loads(json_str)\n",
    "        if not isinstance(boxes_data, list):\n",
    "            return [], []\n",
    "        \n",
    "        # Extract labels and bboxes\n",
    "        labels = []\n",
    "        boxes = []\n",
    "        for item in boxes_data:\n",
    "            if isinstance(item, dict):\n",
    "                label = item.get(\"label\", \"\").strip()\n",
    "                bbox = item.get(\"bbox_2d\", [])\n",
    "                if isinstance(bbox, list) and len(bbox) == 4:\n",
    "                    try:\n",
    "                        bbox = [float(x) for x in bbox]\n",
    "                        boxes.append(bbox)\n",
    "                        labels.append(label)\n",
    "                    except (TypeError, ValueError):\n",
    "                        continue\n",
    "        return labels, boxes\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to decode JSON: {json_str[:100]}... | Error: {e}\")\n",
    "        return [], []\n",
    "\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    \"\"\"Compute IoU between two bounding boxes [x1, y1, x2, y2]\"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "\n",
    "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    unionArea = areaA + areaB - interArea\n",
    "\n",
    "    return interArea / unionArea if unionArea > 0 else 0.0\n",
    "\n",
    "\n",
    "def total_matched_iou_hungarian(gt_boxes, pred_boxes):\n",
    "    if not gt_boxes or not pred_boxes:\n",
    "        return 0.0\n",
    "\n",
    "    M, N = len(gt_boxes), len(pred_boxes)\n",
    "    cost_matrix = np.zeros((M, N))\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            cost_matrix[i, j] = iou(gt_boxes[i], pred_boxes[j])\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n",
    "    matched_ious = cost_matrix[row_ind, col_ind]\n",
    "\n",
    "    return float(np.sum(matched_ious))  \n",
    "\n",
    "\n",
    "results = []\n",
    "file_path = \"lego_dataset_obj_cot_all.jsonl\"  \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Line {line_num}: JSON decode error: {e}\")\n",
    "            continue\n",
    "\n",
    "        predict_str_full = data.get(\"predict\", \"\")\n",
    "        label_str_full = data.get(\"label\", \"\")\n",
    "\n",
    "        #Extract content from <answer> tag\n",
    "        predict_str = extract_answer_content(predict_str_full)\n",
    "        label_str = extract_answer_content(label_str_full)\n",
    "\n",
    "        pr_labels, pr_boxes = parse_json_boxes_and_text(predict_str)\n",
    "        gt_labels, gt_boxes = parse_json_boxes_and_text(label_str)\n",
    "\n",
    "        word_f1 = compute_word_level_f1(gt_labels, pr_labels)\n",
    "        miou = total_matched_iou_hungarian(gt_boxes, pr_boxes)\n",
    "\n",
    "        results.append({\n",
    "            \"word_f1\": word_f1,\n",
    "            \"iou\": miou,\n",
    "            \"gt_labels\": gt_labels,\n",
    "            \"pr_labels\": pr_labels,\n",
    "            \"gt_boxes\": gt_boxes,\n",
    "            \"pr_boxes\": pr_boxes\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "if results:\n",
    "    avg_f1 = np.mean([r[\"word_f1\"] for r in results])\n",
    "    avg_iou = np.mean([r[\"iou\"] for r in results])\n",
    "\n",
    "    print(f\"Average Word-level F1: {avg_f1:.4f}\")\n",
    "    print(f\"Average Matched IoU:    {avg_iou:.4f}\")\n",
    "    print(f\"Total Examples:         {len(results)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318ff30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8ee4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
