{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a583f5f",
   "metadata": {},
   "source": [
    "# DATA WRANGLING PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db7243",
   "metadata": {},
   "source": [
    "### Download Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e57b48-d2b8-436a-92ff-4372274d1bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_folder = snapshot_download(\n",
    "    repo_id=\"PPPPPeter/arta\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"./\",            \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd4f0c",
   "metadata": {},
   "source": [
    "### Define Dataframe from Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502a789-2ec9-43ca-96ac-2c63d740ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePath\n",
    "from collections import Counter, defaultdict\n",
    "import json, re, random\n",
    "\n",
    "_step_re = re.compile(r'[_-](\\d{1,4})_step', re.I)\n",
    "_eop_re  = re.compile(r'[_-](\\d{1,4}).*?eop',  re.I)\n",
    "\n",
    "def _canon(p: str | None) -> str | None:\n",
    "    \"\"\"Return the path inside the first “lego/” (inclusive of inner lego-xxx folder).\"\"\"\n",
    "    if not p:\n",
    "        return None\n",
    "    parts = PurePath(p).parts\n",
    "    try:\n",
    "        first = parts.index(\"lego\")\n",
    "    except ValueError:\n",
    "        return str(p)\n",
    "    # skip the outer 'lego/' if next part is the manual directory\n",
    "    if first + 1 < len(parts) and parts[first+1].startswith(\"lego-\"):\n",
    "        first += 1\n",
    "    return str(PurePath(*parts[first:]))\n",
    "\n",
    "class LegoVLMDataset:\n",
    "    \"\"\"Unified LEGO-VLM dataset (json_data ∪ qwen_data) + prev_instruction.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, keep_invalid=False, seed=42):\n",
    "        self.root = Path(root_dir)\n",
    "        self.meta = self._load_json_data(self.root / \"json_data\")\n",
    "        self.eop_caption = {}\n",
    "        for key, info in self.meta.items():\n",
    "            sc = info.get(\"step_class\",\"\") or \"\"\n",
    "            if \"eop\" in sc.lower() and info.get(\"step_num\"):\n",
    "                manual = PurePath(key).parts[0]\n",
    "                self.eop_caption[(manual, int(info[\"step_num\"]))] = info[\"text\"]\n",
    "\n",
    "        self.rows = self._load_qwen_rows(self.root / \"qwen_data\")\n",
    "        self.rows = [\n",
    "            r for r in self.rows\n",
    "            if not (r[\"task\"] == \"object\" and \"<p>ImageContent</p>\" in r[\"response\"])\n",
    "        ]\n",
    "        self.data = self._merge()\n",
    "        if not keep_invalid:\n",
    "            self._filter_table()\n",
    "        self._add_prev_instruction()\n",
    "        random.Random(seed).shuffle(self.data)\n",
    "\n",
    "    def __len__(self):   return len(self.data)\n",
    "    def __getitem__(self,i): return self.data[i]\n",
    "    def stats(self):\n",
    "        c = Counter(r[\"task\"] for r in self.data)\n",
    "        return dict(total=len(self.data), **c)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_json_data(root: Path):\n",
    "        out = {}\n",
    "        for js in root.rglob(\"*.json\"):\n",
    "            d = json.loads(js.read_text(encoding=\"utf-8\"))\n",
    "            manual_id = d.get(\"manual_id\", \"unknown\")\n",
    "            for inst in d.get(\"instructions\", []):\n",
    "                vlm = inst.get(\"VLM\") or {}\n",
    "                raw = vlm.get(\"img_path\")\n",
    "                if not raw or \"lego/\" not in raw:\n",
    "                    continue\n",
    "                key = _canon(raw[raw.index(\"lego/\"):])\n",
    "                txt = inst.get(\"text\", [])\n",
    "                prompt = \" \".join(txt).strip() if isinstance(txt, list) else str(txt).strip()\n",
    "                out[key] = {\n",
    "                    \"step_class\": vlm.get(\"step_class\"),\n",
    "                    \"step_num\": (None if vlm.get(\"step_num\") is None else str(vlm[\"step_num\"])),\n",
    "                    \"text\": prompt,\n",
    "                    \"instruction_id\": inst.get(\"instruction_id\"), \n",
    "                    \"manual_id\": manual_id  \n",
    "                }\n",
    "        return out\n",
    "\n",
    "    def _load_qwen_rows(self, root: Path):\n",
    "        import re\n",
    "        cfg = {\n",
    "            \"grounding\": (\"grounding_qwen.jsonl\", \"lego\"),\n",
    "            \"object\":    (\"object_qwen_new.jsonl\",    \"object_dataset/lego\"),\n",
    "            \"state\":     (\"state_qwen.jsonl\",     \"step_dataset/lego\"),\n",
    "        }\n",
    "        fake_suffix = re.compile(r\"_fake\\d+\")\n",
    "        rows = []\n",
    "        for task, (fname, prefix) in cfg.items():\n",
    "            text = (root / fname).read_text(encoding=\"utf-8\")\n",
    "            chunks, buf, depth, instr, esc = [], \"\", 0, False, False\n",
    "            for ch in text:\n",
    "                buf += ch\n",
    "                if instr:\n",
    "                    esc = (not esc) if ch == \"\\\\\" else False\n",
    "                    if ch == '\"' and not esc:\n",
    "                        instr = False\n",
    "                else:\n",
    "                    if ch == '\"':      instr = True\n",
    "                    elif ch == '[':    depth += 1\n",
    "                    elif ch == ']':    depth -= 1\n",
    "                if depth == 0 and buf.strip():\n",
    "                    chunks.append(buf.strip())\n",
    "                    buf = \"\"\n",
    "            for block in chunks:\n",
    "                turns = json.loads(block)\n",
    "                user = next((t for t in turns if t[\"role\"].lower() == \"user\"), {})\n",
    "                asst = next((t for t in turns if t[\"role\"].lower() == \"assistant\"), {})\n",
    "                img_uri = next((c[\"image\"] for c in user.get(\"content\", [])\n",
    "                                if c[\"type\"] == \"image\"), None)\n",
    "                if not img_uri:\n",
    "                    continue\n",
    "                rel = img_uri.removeprefix(\"file://\").split(\"lego/\", 1)[-1]\n",
    "                img_path = self.root / prefix / rel\n",
    "                if not img_path.exists() and \"fake\" in img_path.name and \"_fake\" not in img_path.name:\n",
    "                    alt = img_path.with_name(img_path.name.replace(\"fake\", \"_fake\", 1))\n",
    "                    if alt.exists():\n",
    "                        img_path = alt\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "                prompt  = next(c[\"text\"] for c in user[\"content\"] if c[\"type\"] == \"text\")\n",
    "                response = next((c[\"text\"] for c in asst.get(\"content\", []) if c[\"type\"] == \"text\"), \"\").strip()\n",
    "\n",
    "                import ast \n",
    "                if task == \"object\":\n",
    "                    if response.startswith('[') and response.endswith(']'):\n",
    "                        try:\n",
    "                            parsed_response = ast.literal_eval(response)\n",
    "\n",
    "                            response = json.dumps(parsed_response, separators=(',', ':'), ensure_ascii=False)\n",
    "                        except (ValueError, SyntaxError, TypeError):\n",
    "                            pass\n",
    "                skip_row = False\n",
    "                if task == \"object\":\n",
    "                    try:\n",
    "                        parsed_response = json.loads(response)\n",
    "                        if isinstance(parsed_response, list):\n",
    "                            if any(item.get(\"label\") == \"ImageContent\" for item in parsed_response):\n",
    "                                skip_row = True\n",
    "                    except (json.JSONDecodeError, TypeError):\n",
    "                        pass\n",
    "                if skip_row:\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"task\":       task,\n",
    "                    \"image_path\": str(img_path),\n",
    "                    \"prompt\":     prompt,\n",
    "                    \"response\":   response, \n",
    "                })\n",
    "                if task == \"state\" and \"_fake\" in img_path.name:\n",
    "                    parts = Path(img_path).parts\n",
    "                    try:\n",
    "                        i = parts.index(\"lego\")\n",
    "                        manual = parts[i+1]\n",
    "                    except ValueError:\n",
    "                        manual = None\n",
    "                    if manual:\n",
    "                        orig_name = fake_suffix.sub(\"\", img_path.name)\n",
    "                        orig_path = self.root / \"lego\" / manual / \"images\" / orig_name\n",
    "                        if orig_path.exists():\n",
    "                            rows.append({\n",
    "                                \"task\":       task,\n",
    "                                \"image_path\": str(orig_path),\n",
    "                                \"prompt\":     prompt,\n",
    "                                \"response\":   response,\n",
    "                            })\n",
    "        return rows\n",
    "\n",
    "\n",
    "    def _merge(self):\n",
    "        merged = []\n",
    "        for r in self.rows:\n",
    "            key   = _canon(r[\"image_path\"])\n",
    "            extra = self.meta.get(key, {})\n",
    "            row   = r | extra\n",
    "            fn = Path(r[\"image_path\"]).name.lower()\n",
    "            if row.get(\"step_class\") is None:\n",
    "                if \"_step_\" in fn:\n",
    "                    row[\"step_class\"]=\"step\"\n",
    "                    row[\"step_num\"]  = row.get(\"step_num\") or _step_re.search(fn).group(1)\n",
    "                elif \"eop\" in fn:\n",
    "                    row[\"step_class\"]=\"eop\"\n",
    "                    row[\"step_num\"]  = row.get(\"step_num\") or _eop_re.search(fn).group(1)\n",
    "            merged.append(row)\n",
    "        seen,out=set(),[]\n",
    "        for r in merged:\n",
    "            k=(r[\"image_path\"],r[\"task\"])\n",
    "            if k not in seen:\n",
    "                seen.add(k); out.append(r)\n",
    "        return out\n",
    "\n",
    "    def _filter_table(self):\n",
    "        good=[]\n",
    "        for r in self.data:\n",
    "            cls,task,num=r.get(\"step_class\"),r[\"task\"],r.get(\"step_num\")\n",
    "            if task==\"grounding\" and (cls!=\"step\" or num is None): continue\n",
    "            if task==\"object\"    and cls!=\"eop\": continue\n",
    "            if task==\"state\"     and cls not in (\"step\",\"eop\",\"fullscreeneop\"):  continue \n",
    "            good.append(r)\n",
    "        self.data=good\n",
    "    \n",
    "    def _add_prev_instruction(self):\n",
    "        \"\"\"Attach `prev_instruction` from self.meta using instruction_id.\"\"\"\n",
    "        for r in self.data:\n",
    "            prev_txt = \"\"\n",
    "            if r.get(\"step_class\") != \"step\" or r.get(\"instruction_id\") is None:\n",
    "                r[\"prev_instruction\"] = \"\"\n",
    "                continue\n",
    "    \n",
    "            manual_id = r.get(\"manual_id\")\n",
    "            curr_id = r[\"instruction_id\"]\n",
    "            prev_id = curr_id - 1\n",
    "    \n",
    "            if prev_id < 0:\n",
    "                r[\"prev_instruction\"] = \"\"\n",
    "                continue\n",
    "    \n",
    "            for key, meta in self.meta.items():\n",
    "                if meta.get(\"manual_id\") == manual_id and meta.get(\"instruction_id\") == prev_id:\n",
    "                    prev_txt = meta.get(\"text\", \"\")\n",
    "                    break\n",
    "    \n",
    "            r[\"prev_instruction\"] = prev_txt.strip()\n",
    "'''\n",
    "# usage check:\n",
    "ds = LegoVLMDataset(\"ARTA_LEGO\")\n",
    "print(\"stats:\", ds.stats())\n",
    "cnt = sum(1 for r in ds.data if r[\"prev_instruction\"])\n",
    "print(f\"{cnt} / {len(ds.data)} have prev_instruction\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db0ab3-8c52-4ce3-b626-3c9d5ead560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re \n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "COT_INSTRUCTION = \"{Question} First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.\"\n",
    "COT_INSTRUCTION_STATE = \"{Question} First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags.\"\n",
    "\n",
    "# Format directives for each task\n",
    "fmt = {\n",
    "    \"grounding\": \"\",\n",
    "    \"object\": (\n",
    "        \"Providing the positions in the format: \"\n",
    "        \"[{\\\"bbox_2d\\\": [x1, y1, x2, y2], \\\"label\\\": \\\"1 object name\\\"}]\"\n",
    "        \"x1 and y1 for the top-left corner. \"\n",
    "        \"x2 and y2 for the bottom-right corner.\"\n",
    "    ),\n",
    "    \"state\": \"Just tell me Yes or No.\"\n",
    "}\n",
    "\n",
    "def add_fmt(txt, task):\n",
    "    suffix = fmt.get(task, \"\").strip()\n",
    "    return txt if (not suffix or txt.strip().endswith(suffix)) else f\"{txt} {suffix}\"\n",
    "\n",
    "def manual_id(p: str | Path) -> str:\n",
    "    parts = Path(p).parts\n",
    "    try:\n",
    "        return parts[parts.index(\"lego\") + 1]\n",
    "    except (ValueError, IndexError):\n",
    "        return \"unknown\"\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "ds = LegoVLMDataset(\"ARTA_LEGO\")\n",
    "\n",
    "by_manual_full = defaultdict(list) \n",
    "by_manual_state_full = defaultdict(list) \n",
    "by_manual = defaultdict(list) \n",
    "for r in ds.data:\n",
    "    mid = manual_id(r[\"image_path\"])\n",
    "    if r[\"task\"] == \"object\" and r.get(\"step_class\") == \"eop\":\n",
    "        by_manual_full[mid].append(r)\n",
    "    if r[\"task\"] == \"state\" and r.get(\"step_class\") == \"step\":\n",
    "        by_manual_state_full[mid].append(r)\n",
    "    if r[\"task\"] == \"grounding\" and r.get(\"step_class\") == \"step\":\n",
    "        by_manual[mid].append(r)\n",
    "\n",
    "def count_objs(resp_data):\n",
    "    \"\"\"Count number of bounding boxes in response.\"\"\"\n",
    "    if isinstance(resp_data, str):\n",
    "        try:\n",
    "            data = json.loads(resp_data)\n",
    "            if isinstance(data, list):\n",
    "                return len(data)\n",
    "        except json.JSONDecodeError:\n",
    "            return len(re.findall(r\"<p>[^<]+</p>\", resp_data))\n",
    "    elif isinstance(resp_data, list):\n",
    "        return len(resp_data)\n",
    "    return 0\n",
    "\n",
    "def pick_demo(rec):\n",
    "    task = rec[\"task\"]\n",
    "    manual = manual_id(rec[\"image_path\"])\n",
    "    if task == \"grounding\":\n",
    "        try:\n",
    "            tgt = int(rec[\"step_num\"])\n",
    "        except (TypeError, ValueError):\n",
    "            return None, \"none\"\n",
    "        pool = by_manual.get(manual, [])\n",
    "        for c in pool:\n",
    "            if (c[\"step_class\"] == \"step\" and c.get(\"step_num\") and\n",
    "                int(c[\"step_num\"]) == tgt - 1 and c is not rec):\n",
    "                return c, \"success\"\n",
    "        return None, \"none\"\n",
    "    elif task == \"object\":\n",
    "        try:\n",
    "            gt_n = len(json.loads(rec[\"response\"]))\n",
    "        except json.JSONDecodeError:\n",
    "            gt_n = count_objs(rec[\"response\"])\n",
    "        candidates = [r for r in by_manual_full[manual] if r is not rec]\n",
    "        cand_same_count = [r for r in candidates if count_objs(r[\"response\"]) == gt_n]\n",
    "        if cand_same_count:\n",
    "            demo = random.choice(cand_same_count)\n",
    "            return demo, \"success\"\n",
    "        elif candidates:\n",
    "            demo = random.choice(candidates)\n",
    "            return demo, \"fallback\"\n",
    "        return None, \"none\"\n",
    "    elif task == \"state\":\n",
    "        pool = [r for r in ds.data if r[\"task\"] == \"state\" and\n",
    "                manual_id(r[\"image_path\"]) == manual and r is not rec]\n",
    "        real_cands = [r for r in pool if \"_fake\" not in r[\"image_path\"]]\n",
    "        fake_cands = [r for r in pool if \"_fake\" in r[\"image_path\"]]\n",
    "        if not real_cands or not fake_cands:\n",
    "            return None, \"none\"\n",
    "        demo_yes = random.choice(real_cands)\n",
    "        real_base = Path(demo_yes[\"image_path\"]).stem\n",
    "        related_fakes = [r for r in fake_cands if Path(r[\"image_path\"]).stem.startswith(f\"{real_base}_fake\")]\n",
    "        demo_no = random.choice(related_fakes) if related_fakes else random.choice(fake_cands)\n",
    "        return [demo_yes, demo_no], \"pair\"\n",
    "    return None, \"none\"\n",
    "\n",
    "def stratified_sample(items, key_fn, frac, rng=random):\n",
    "    buckets = defaultdict(list)\n",
    "    for x in items:\n",
    "        k = key_fn(x)\n",
    "        buckets[k].append(x)\n",
    "    out = []\n",
    "    for b in buckets.values():\n",
    "        k = max(1, int(round(len(b) * frac)))\n",
    "        out.extend(rng.sample(b, k))\n",
    "    return out\n",
    "\n",
    "others = [r for r in ds.data if r[\"task\"] in (\"grounding\", \"object\")]\n",
    "_sub_others_25 = stratified_sample(others, key_fn=lambda r: r[\"task\"], frac=0.25, rng=random)\n",
    "_sub_others_set = set((r[\"image_path\"], r[\"task\"]) for r in _sub_others_25)\n",
    "_sub_others_75 = [r for r in others if (r[\"image_path\"], r[\"task\"]) not in _sub_others_set]\n",
    "\n",
    "def orig_key(rec):\n",
    "    fname = Path(rec[\"image_path\"]).name\n",
    "    return re.sub(r\"_fake\\d+(\\.\\w+)$\", r\"\\1\", fname)\n",
    "\n",
    "state_groups = defaultdict(list)\n",
    "for r in ds.data:\n",
    "    if r[\"task\"] == \"state\":\n",
    "        key = orig_key(r)\n",
    "        state_groups[key].append(r)\n",
    "\n",
    "all_keys = list(state_groups.keys())\n",
    "n_25 = max(1, int(round(len(all_keys) * 0.25)))\n",
    "n_5 = max(1, int(round(len(all_keys) * 0.05)))\n",
    "\n",
    "sampled_keys_25 = random.sample(all_keys, n_25)\n",
    "sampled_keys_5 = random.sample(all_keys, n_5)\n",
    "\n",
    "# Build 25% state subset\n",
    "_sub_state_25 = []\n",
    "for key in sampled_keys_25:\n",
    "    _sub_state_25.extend(state_groups[key])\n",
    "\n",
    "# Build 75% state subset \n",
    "_sub_state_75 = []\n",
    "for key in all_keys:\n",
    "    if key not in sampled_keys_25:\n",
    "        _sub_state_75.extend(state_groups[key])\n",
    "\n",
    "# Build 5% state subset for testing\n",
    "_sub_state_5 = []\n",
    "for key in sampled_keys_5:\n",
    "    _sub_state_5.extend(state_groups[key])\n",
    "\n",
    "# Combine splits \n",
    "_sub_25 = _sub_others_25 + _sub_state_25\n",
    "_sub_75 = _sub_others_75 + _sub_state_75\n",
    "_sub_5 = stratified_sample([r for r in ds.data if r[\"task\"] in (\"grounding\", \"object\")],\n",
    "                           key_fn=lambda r: r[\"task\"], frac=0.05, rng=random) + _sub_state_5\n",
    "\n",
    "def debug_split(name, data):\n",
    "    s = [r[\"task\"] for r in data]\n",
    "    c = {t: s.count(t) for t in set(s)}\n",
    "    print(f\"{name}: total={len(data)}, {c}\")\n",
    "\n",
    "debug_split(\"[DEBUG] 25% split\", _sub_25)\n",
    "debug_split(\"[DEBUG] 75% split\", _sub_75)\n",
    "debug_split(\"[DEBUG] 5% split\", _sub_5)\n",
    "\n",
    "_sub_25_cot = []\n",
    "for item in _sub_25:\n",
    "    modified_item = item.copy()\n",
    "    cot_instruction_to_add = COT_INSTRUCTION_STATE if item[\"task\"] == \"state\" else COT_INSTRUCTION\n",
    "    modified_item[\"prompt\"] = f\"{cot_instruction_to_add} {item['prompt'].strip()}\"\n",
    "    _sub_25_cot.append(modified_item)\n",
    "\n",
    "_sub_5_cot = []\n",
    "for item in _sub_5:\n",
    "    modified_item = item.copy()\n",
    "    cot_instruction_to_add = COT_INSTRUCTION_STATE if item[\"task\"] == \"state\" else COT_INSTRUCTION\n",
    "    modified_item[\"prompt\"] = f\"{cot_instruction_to_add} {item['prompt'].strip()}\"\n",
    "    _sub_5_cot.append(modified_item)\n",
    "\n",
    "\n",
    "def build_conversation(row, idx):\n",
    "    task = row[\"task\"]\n",
    "    prompt = row[\"prompt\"].strip() \n",
    "    prev_instruction = row.get(\"prev_instruction\", \"\").strip()\n",
    "\n",
    "    is_cot = COT_INSTRUCTION in prompt or COT_INSTRUCTION_STATE in prompt\n",
    "\n",
    "    if task == \"state\":\n",
    "        base_response = \"Yes\" if \"_fake\" not in row[\"image_path\"] else \"No\"\n",
    "        response = f\"<answer>{base_response}.</answer>\" if is_cot else f\"{base_response}.\"\n",
    "    else: \n",
    "        base_response = row[\"response\"]\n",
    "        response = f\"<answer>{base_response}</answer>\" if is_cot else base_response\n",
    "    conversations = []\n",
    "    all_images = []\n",
    "    demo, stage = pick_demo(row)\n",
    "    if demo is not None:\n",
    "        if isinstance(demo, list): \n",
    "            for d in demo:\n",
    "                d_prev = d.get(\"prev_instruction\", \"\").strip()\n",
    "                d_base_response = \"Yes\" if \"_fake\" not in d[\"image_path\"] else \"No\"\n",
    "\n",
    "                d_response = f\"<answer>{d_base_response}.</answer>\" if is_cot else f\"{d_base_response}.\"\n",
    "                demo_prompt_parts = []\n",
    "                if d[\"task\"] in (\"grounding\", \"state\") and d_prev:\n",
    "                    demo_prompt_parts.append(f\"Previous instruction: {d_prev}\")\n",
    "                demo_prompt_base = add_fmt(d[\"prompt\"], task)\n",
    "                if is_cot and not (COT_INSTRUCTION in demo_prompt_base or COT_INSTRUCTION_STATE in demo_prompt_base):\n",
    "                    demo_cot_instruction = COT_INSTRUCTION_STATE if d[\"task\"] == \"state\" else COT_INSTRUCTION\n",
    "                    demo_prompt_base = f\"{demo_cot_instruction} {demo_prompt_base}\"\n",
    "                demo_prompt_parts.append(demo_prompt_base)\n",
    "                demo_prompt = \"\\n\".join(demo_prompt_parts)\n",
    "                conversations.append({\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": f\"<image>{demo_prompt}\"\n",
    "                })\n",
    "                conversations.append({\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": d_response \n",
    "                })\n",
    "                all_images.append(d[\"image_path\"])\n",
    "        else: \n",
    "            d_prev = demo.get(\"prev_instruction\", \"\").strip()\n",
    "            demo_prompt_parts = []\n",
    "            if demo[\"task\"] in (\"grounding\", \"state\") and d_prev:\n",
    "                demo_prompt_parts.append(f\"Previous instruction: {d_prev}\")\n",
    "            demo_prompt_base = add_fmt(demo[\"prompt\"], task)\n",
    "            if is_cot and not (COT_INSTRUCTION in demo_prompt_base or COT_INSTRUCTION_STATE in demo_prompt_base):\n",
    "                 demo_cot_instruction = COT_INSTRUCTION_STATE if demo[\"task\"] == \"state\" else COT_INSTRUCTION\n",
    "                 demo_prompt_base = f\"{demo_cot_instruction} {demo_prompt_base}\"\n",
    "            demo_prompt_parts.append(demo_prompt_base)\n",
    "            demo_prompt = \"\\n\".join(demo_prompt_parts)\n",
    "            d_base_response = demo[\"response\"]\n",
    "            d_response = f\"<answer>{d_base_response}</answer>\" if is_cot else d_base_response\n",
    "            conversations.append({\n",
    "                \"from\": \"human\",\n",
    "                \"value\": f\"<image>{demo_prompt}\"\n",
    "            })\n",
    "            conversations.append({\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": d_response \n",
    "            })\n",
    "            all_images.append(demo[\"image_path\"])\n",
    "\n",
    "    main_prompt_parts = []\n",
    "    if task in (\"grounding\", \"state\") and prev_instruction:\n",
    "        main_prompt_parts.append(f\"Previous instruction: {prev_instruction}\")\n",
    "    main_prompt_parts.append(prompt) \n",
    "    full_main_prompt = \"\\n\".join(main_prompt_parts)\n",
    "    if not is_cot:\n",
    "        directive = fmt.get(task, \"\").strip()\n",
    "        if directive and not full_main_prompt.lower().endswith(directive.lower()):\n",
    "             main_prompt_parts.append(directive)\n",
    "    main_prompt = \"\\n\".join(main_prompt_parts)\n",
    "    conversations.append({\n",
    "        \"from\": \"human\",\n",
    "        \"value\": f\"<image>{main_prompt}\"\n",
    "    })\n",
    "    conversations.append({\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": response\n",
    "    })\n",
    "    all_images.append(row[\"image_path\"])\n",
    "    return {\n",
    "        \"id\": idx,\n",
    "        \"conversations\": conversations,\n",
    "        \"images\": all_images\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Precomputing conversations for 25% split...\")\n",
    "data_25 = [build_conversation(row, idx) for idx, row in enumerate(_sub_25)]\n",
    "\n",
    "print(\"Precomputing conversations for 75% split...\")\n",
    "data_75 = [build_conversation(row, idx + len(data_25)) for idx, row in enumerate(_sub_75)]\n",
    "\n",
    "print(\"Precomputing conversations for 5% split...\")\n",
    "data_5 = [build_conversation(row, idx + len(data_25) + len(data_75)) for idx, row in enumerate(_sub_5)]\n",
    "\n",
    "print(\"Precomputing conversations for 25% CoT split...\")\n",
    "start_id_25_cot = len(data_25) + len(data_75) + len(data_5)\n",
    "data_25_cot = [build_conversation(row, idx + start_id_25_cot) for idx, row in enumerate(_sub_25_cot)]\n",
    "\n",
    "print(\"Precomputing conversations for 5% CoT split...\")\n",
    "start_id_5_cot = start_id_25_cot + len(data_25_cot)\n",
    "data_5_cot = [build_conversation(row, idx + start_id_5_cot) for idx, row in enumerate(_sub_5_cot)]\n",
    "\n",
    "\n",
    "# === Split by Task and Save ===\n",
    "def save_by_task(data, prefix):\n",
    "    by_task = {\"grounding\": [], \"object\": [], \"state\": []}\n",
    "    for item in data:\n",
    "        last_turn = item[\"conversations\"][-1]\n",
    "        first_turn = item[\"conversations\"][0]\n",
    "        if last_turn[\"from\"] == \"gpt\":\n",
    "            resp = last_turn[\"value\"].strip().lower()\n",
    "\n",
    "            first_prompt_lower = first_turn[\"value\"].lower()\n",
    "            if \"yes.\" in resp or \"no.\" in resp:\n",
    "                by_task[\"state\"].append(item)\n",
    "\n",
    "            elif any(\"bbox_2d\" in tok for tok in re.split(r'[{}[\\],\\s]+', last_turn[\"value\"])) or \\\n",
    "                 (\"object name\" in first_prompt_lower and \"bbox_2d\" in first_prompt_lower):\n",
    "                 by_task[\"object\"].append(item)\n",
    "            else:\n",
    "                by_task[\"grounding\"].append(item)\n",
    "\n",
    "    task_suffix = {\"grounding\": \"grnd\", \"object\": \"obj\", \"state\": \"state\"}\n",
    "    for task, short in task_suffix.items():\n",
    "        path = f\"converted_dataset_{prefix}_{short}.jsonl\"\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in by_task[task]:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"Saved {len(by_task[task])} entries to {path}\")\n",
    "\n",
    "save_by_task(data_25, \"25\")\n",
    "save_by_task(data_5, \"5\")\n",
    "\n",
    "save_by_task(data_25_cot, \"25_cot\")\n",
    "save_by_task(data_5_cot, \"5_cot\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
