{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a583f5f",
   "metadata": {},
   "source": [
    "# Qwen 2.5-VL Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db7243",
   "metadata": {},
   "source": [
    "### Download Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e57b48-d2b8-436a-92ff-4372274d1bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_folder = snapshot_download(\n",
    "    repo_id=\"PPPPPeter/arta\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"./\",            \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets pillow accelerate \n",
    "#%pip install --upgrade pip setuptools wheel hatchling ninja packaging\n",
    "#%pip install --no-binary \":all:\" --no-build-isolation -vvv flash-attn 2>&1 | tee $LOG\n",
    "%pip install --upgrade --index-url https://download.pytorch.org/whl/cu118 \\\n",
    "            torch==2.1.0\n",
    "%pip install --upgrade transformers==4.51.0\n",
    "%pip install \\ https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.5/flash_attn-2.5.5+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
    "%pip install qwen-vl-utils\n",
    "%pip install tqdm\n",
    "#%pip install accelerate>=0.26.0\n",
    "# -------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    ")\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd4f0c",
   "metadata": {},
   "source": [
    "### Define Dataframe from Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502a789-2ec9-43ca-96ac-2c63d740ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePath\n",
    "from collections import Counter, defaultdict\n",
    "import json, re, random\n",
    "\n",
    "_step_re = re.compile(r'[_-](\\d{1,4})_step', re.I)\n",
    "_eop_re  = re.compile(r'[_-](\\d{1,4}).*?eop',  re.I)\n",
    "\n",
    "def _canon(p: str | None) -> str | None:\n",
    "    \"\"\"Return the path inside the first “lego/” (inclusive of inner lego-xxx folder).\"\"\"\n",
    "    if not p:\n",
    "        return None\n",
    "    parts = PurePath(p).parts\n",
    "    try:\n",
    "        first = parts.index(\"lego\")\n",
    "    except ValueError:\n",
    "        return str(p)\n",
    "    # skip the outer 'lego/' if next part is the manual directory\n",
    "    if first + 1 < len(parts) and parts[first+1].startswith(\"lego-\"):\n",
    "        first += 1\n",
    "    return str(PurePath(*parts[first:]))\n",
    "\n",
    "class LegoVLMDataset:\n",
    "    \"\"\"Unified LEGO-VLM dataset (json_data ∪ qwen_data) + prev_instruction.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, keep_invalid=False, seed=42):\n",
    "        self.root = Path(root_dir)\n",
    "        self.meta = self._load_json_data(self.root / \"json_data\")\n",
    "        self.eop_caption = {}\n",
    "        for key, info in self.meta.items():\n",
    "            sc = info.get(\"step_class\",\"\") or \"\"\n",
    "            if \"eop\" in sc.lower() and info.get(\"step_num\"):\n",
    "                manual = PurePath(key).parts[0]\n",
    "                self.eop_caption[(manual, int(info[\"step_num\"]))] = info[\"text\"]\n",
    "\n",
    "        self.rows = self._load_qwen_rows(self.root / \"qwen_data\")\n",
    "        self.rows = [\n",
    "            r for r in self.rows\n",
    "            if not (r[\"task\"] == \"object\" and \"<p>ImageContent</p>\" in r[\"response\"])\n",
    "        ]\n",
    "        self.data = self._merge()\n",
    "        if not keep_invalid:\n",
    "            self._filter_table()\n",
    "        self._add_prev_instruction()\n",
    "        random.Random(seed).shuffle(self.data)\n",
    "\n",
    "    def __len__(self):   return len(self.data)\n",
    "    def __getitem__(self,i): return self.data[i]\n",
    "    def stats(self):\n",
    "        c = Counter(r[\"task\"] for r in self.data)\n",
    "        return dict(total=len(self.data), **c)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_json_data(root: Path):\n",
    "        out = {}\n",
    "        for js in root.rglob(\"*.json\"):\n",
    "            d = json.loads(js.read_text(encoding=\"utf-8\"))\n",
    "            for inst in d.get(\"instructions\", []):\n",
    "                vlm = inst.get(\"VLM\") or {}\n",
    "                raw = vlm.get(\"img_path\")\n",
    "                if not raw or \"lego/\" not in raw:\n",
    "                    continue\n",
    "                key = _canon(raw[raw.index(\"lego/\"):])\n",
    "                # join the JSON \"text\":[…] list into one prompt string\n",
    "                txt = inst.get(\"text\", [])\n",
    "                prompt = \" \".join(txt).strip() if isinstance(txt, list) else str(txt).strip()\n",
    "                out[key] = {\n",
    "                    \"step_class\": vlm.get(\"step_class\"),\n",
    "                    \"step_num\"  : (None if vlm.get(\"step_num\") is None\n",
    "                                   else str(vlm[\"step_num\"])),\n",
    "                    \"text\"      : prompt,\n",
    "                }\n",
    "        return out\n",
    "\n",
    "    def _load_qwen_rows(self, root: Path):\n",
    "        import re\n",
    "        cfg = {\n",
    "            \"grounding\": (\"grounding_qwen.jsonl\", \"lego\"),\n",
    "            \"object\":    (\"object_qwen.jsonl\",    \"object_dataset/lego\"),\n",
    "            \"state\":     (\"state_qwen.jsonl\",     \"step_dataset/lego\"),\n",
    "        }\n",
    "        fake_suffix = re.compile(r\"_fake\\d+\")\n",
    "        rows = []\n",
    "        for task, (fname, prefix) in cfg.items():\n",
    "            text = (root / fname).read_text(encoding=\"utf-8\")\n",
    "            # --- split JSONL-of-lists into blocks ---\n",
    "            chunks, buf, depth, instr, esc = [], \"\", 0, False, False\n",
    "            for ch in text:\n",
    "                buf += ch\n",
    "                if instr:\n",
    "                    esc = (not esc) if ch == \"\\\\\" else False\n",
    "                    if ch == '\"' and not esc:\n",
    "                        instr = False\n",
    "                else:\n",
    "                    if ch == '\"':      instr = True\n",
    "                    elif ch == '[':    depth += 1\n",
    "                    elif ch == ']':    depth -= 1\n",
    "                if depth == 0 and buf.strip():\n",
    "                    chunks.append(buf.strip())\n",
    "                    buf = \"\"\n",
    "            # --- parse each dialog block ---\n",
    "            for block in chunks:\n",
    "                turns = json.loads(block)\n",
    "                user = next((t for t in turns if t[\"role\"].lower() == \"user\"), {})\n",
    "                asst = next((t for t in turns if t[\"role\"].lower() == \"assistant\"), {})\n",
    "                # get the uploaded image URI\n",
    "                img_uri = next((c[\"image\"] for c in user.get(\"content\", [])\n",
    "                                if c[\"type\"] == \"image\"), None)\n",
    "                if not img_uri:\n",
    "                    continue\n",
    "                # resolve to file on disk\n",
    "                rel = img_uri.removeprefix(\"file://\").split(\"lego/\", 1)[-1]\n",
    "                img_path = self.root / prefix / rel\n",
    "                if not img_path.exists() and \"fake\" in img_path.name and \"_fake\" not in img_path.name:\n",
    "                    alt = img_path.with_name(img_path.name.replace(\"fake\", \"_fake\", 1))\n",
    "                    if alt.exists():\n",
    "                        img_path = alt\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "                # prompt & response text\n",
    "                prompt  = next(c[\"text\"] for c in user[\"content\"] if c[\"type\"] == \"text\")\n",
    "                response = next((c[\"text\"] for c in asst.get(\"content\", []) if c[\"type\"] == \"text\"), \"\").strip()\n",
    "                rows.append({\n",
    "                    \"task\":       task,\n",
    "                    \"image_path\": str(img_path),\n",
    "                    \"prompt\":     prompt,\n",
    "                    \"response\":   response,\n",
    "                })\n",
    "                if task == \"state\" and \"_fake\" in img_path.name:\n",
    "                    parts = Path(img_path).parts\n",
    "                    try:\n",
    "                        i = parts.index(\"lego\")\n",
    "                        manual = parts[i+1]\n",
    "                    except ValueError:\n",
    "                        manual = None\n",
    "                    if manual:\n",
    "                        orig_name = fake_suffix.sub(\"\", img_path.name)\n",
    "                        orig_path = self.root / \"lego\" / manual / \"images\" / orig_name\n",
    "                        if orig_path.exists():\n",
    "                            rows.append({\n",
    "                                \"task\":       task,\n",
    "                                \"image_path\": str(orig_path),\n",
    "                                \"prompt\":     prompt,\n",
    "                                \"response\":   response,\n",
    "                            })\n",
    "        return rows\n",
    "\n",
    "\n",
    "    def _merge(self):\n",
    "        merged = []\n",
    "        for r in self.rows:\n",
    "            key   = _canon(r[\"image_path\"])\n",
    "            extra = self.meta.get(key, {})\n",
    "            row   = r | extra\n",
    "            fn = Path(r[\"image_path\"]).name.lower()\n",
    "            if row.get(\"step_class\") is None:\n",
    "                if \"_step_\" in fn:\n",
    "                    row[\"step_class\"]=\"step\"\n",
    "                    row[\"step_num\"]  = row.get(\"step_num\") or _step_re.search(fn).group(1)\n",
    "                elif \"eop\" in fn:\n",
    "                    row[\"step_class\"]=\"eop\"\n",
    "                    row[\"step_num\"]  = row.get(\"step_num\") or _eop_re.search(fn).group(1)\n",
    "            merged.append(row)\n",
    "        seen,out=set(),[]\n",
    "        for r in merged:\n",
    "            k=(r[\"image_path\"],r[\"task\"])\n",
    "            if k not in seen:\n",
    "                seen.add(k); out.append(r)\n",
    "        return out\n",
    "\n",
    "    def _filter_table(self):\n",
    "        good=[]\n",
    "        for r in self.data:\n",
    "            cls,task,num=r.get(\"step_class\"),r[\"task\"],r.get(\"step_num\")\n",
    "            if task==\"grounding\" and (cls!=\"step\" or num is None): continue\n",
    "            if task==\"object\"    and cls!=\"eop\": continue\n",
    "            if task==\"state\"     and cls not in (\"step\",\"eop\",\"fullscreeneop\"):  continue \n",
    "            good.append(r)\n",
    "        self.data=good\n",
    "\n",
    "    def _add_prev_instruction(self):\n",
    "        \"\"\"Attach `prev_instruction` from self.meta or self.eop_caption.\"\"\"\n",
    "        for r in self.data:\n",
    "            prev_txt=\"\"\n",
    "            if r.get(\"step_class\")==\"step\" and r.get(\"step_num\"):\n",
    "                canon = _canon(r[\"image_path\"])\n",
    "                n0    = int(r[\"step_num\"])-1\n",
    "                pad   = len(r[\"step_num\"])\n",
    "                # first try the exact meta‐key\n",
    "                key0 = canon.replace(f\"_{r['step_num']}_step\",\n",
    "                                     f\"_{str(n0).zfill(pad)}_eop\")\n",
    "                prev_txt = self.meta.get(key0,{}).get(\"text\",\"\") or \"\"\n",
    "                if not prev_txt:\n",
    "                    # fall back to our in‐memory table\n",
    "                    manual = PurePath(canon).parts[0]\n",
    "                    prev_txt = self.eop_caption.get((manual,n0),\"\")\n",
    "            r[\"prev_instruction\"]=prev_txt\n",
    "'''\n",
    "# usage check:\n",
    "ds = LegoVLMDataset(\"ARTA_LEGO\")\n",
    "print(\"stats:\", ds.stats())\n",
    "cnt = sum(1 for r in ds.data if r[\"prev_instruction\"])\n",
    "print(f\"{cnt} / {len(ds.data)} have prev_instruction\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbdccce",
   "metadata": {},
   "source": [
    "### Example picking helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd4e57-ac70-40a2-8c6e-22fb73948263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path, PurePath\n",
    "from collections import defaultdict, Counter\n",
    "import json, random, os, gc, re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info      # noqa: F401\n",
    "\n",
    "#dataset 25 % subsample\n",
    "ds = LegoVLMDataset(\"ARTA_LEGO\")          \n",
    "print(\"Full dataset :\", ds.stats())\n",
    "def stratified_sample(items, key_fn, frac, rng=random):\n",
    "    buckets = defaultdict(list)\n",
    "    for x in items:\n",
    "        buckets[key_fn(x)].append(x)\n",
    "    out = []\n",
    "    for b in buckets.values():\n",
    "        k = max(1, int(round(len(b) * frac)))\n",
    "        out.extend(rng.sample(b, k))\n",
    "    return out\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# A) sample grounding & object as before\n",
    "others = [r for r in ds.data if r[\"task\"] in (\"grounding\",\"object\")]\n",
    "_sub_others = stratified_sample(others, key_fn=lambda r: r[\"task\"], frac=0.25, rng=random)\n",
    "\n",
    "# B) group all state‐frames (3 fake + 1 real) by “orig” filename\n",
    "def orig_key(rec):\n",
    "    fname = Path(rec[\"image_path\"]).name\n",
    "    return re.sub(r\"_fake\\d+(\\.\\w+)$\", r\"\\1\", fname)\n",
    "\n",
    "state_groups = defaultdict(list)\n",
    "for r in ds.data:\n",
    "    if r[\"task\"] == \"state\":\n",
    "        state_groups[orig_key(r)].append(r)\n",
    "\n",
    "# C) sample 25% of these state‐groups\n",
    "all_keys = list(state_groups.keys())\n",
    "n_groups = max(1, int(round(len(all_keys) * 0.25)))\n",
    "sampled_keys = random.sample(all_keys, n_groups)\n",
    "\n",
    "# D) flatten each sampled group back into its 4 records\n",
    "_sub_state = []\n",
    "for key in sampled_keys:\n",
    "    _sub_state.extend(state_groups[key])\n",
    "\n",
    "# DEBUG: verify 1 Yes : 3 No ratio\n",
    "num_yes = sum(1 for r in _sub_state if \"_fake\" not in r[\"image_path\"])\n",
    "num_no  = sum(1 for r in _sub_state if \"_fake\" in r[\"image_path\"])\n",
    "ratio   = num_no / num_yes if num_yes else float(\"inf\")\n",
    "print(f\"[DEBUG] State split: groups={len(sampled_keys)}, yes={num_yes}, no={num_no}, ratio={ratio:.2f}\")\n",
    "\n",
    "# E) combine all tasks\n",
    "_sub = _sub_others + _sub_state\n",
    "print(\"25 % split   :\", Counter(r[\"task\"] for r in _sub))\n",
    "\n",
    "buckets = defaultdict(list)\n",
    "for r in _sub:\n",
    "    buckets[r[\"task\"]].append(r)\n",
    "\n",
    "probe, rest = [], []\n",
    "while any(buckets.values()):\n",
    "    for t in (\"grounding\", \"object\", \"state\"):\n",
    "        if buckets[t]:\n",
    "            tgt = probe if len([x for x in probe if x[\"task\"] == t]) < 2 else rest\n",
    "            tgt.append(buckets[t].pop())\n",
    "records = probe + rest           \n",
    "\n",
    "def manual_id(p: str | Path) -> str:\n",
    "    parts = Path(p).parts\n",
    "    try:\n",
    "        return parts[parts.index(\"lego\") + 1]\n",
    "    except (ValueError, IndexError):\n",
    "        return \"unknown\"\n",
    "\n",
    "by_manual = defaultdict(list)\n",
    "for r in records:\n",
    "    by_manual[manual_id(r[\"image_path\"])].append(r)\n",
    "\n",
    "vlm_map = ds.meta                    \n",
    "\n",
    "fmt = {\n",
    "    \"grounding\": \"\",\n",
    "    \"object\": (\"Providing the positions in the format: \"\n",
    "               \"<p>object</p> {<Xleft><Ytop><Xright><Ybottom>} \"\n",
    "               \"with X and Y coordinates normalized to [0,100].\"\n",
    "               \"<Xleft> and <Ytop> for the top-left corner. \"\n",
    "               \"<Xright> and <Ybottom> for the bottom-right corner. \"),\n",
    "    \"state\": \"Just tell me Yes or No.\"\n",
    "}\n",
    "\n",
    "def add_fmt(txt, task):\n",
    "    suffix = fmt[task]\n",
    "    return txt if (not suffix or txt.strip().endswith(suffix)) else f\"{txt} {suffix}\"\n",
    "\n",
    "def same_dataset(a, b):\n",
    "    return Path(a[\"image_path\"]).parts[1] == Path(b[\"image_path\"]).parts[1]\n",
    "\n",
    "def immediate_prev_step(q, pool):\n",
    "    try: tgt = int(q[\"step_num\"])\n",
    "    except (TypeError, ValueError): return None\n",
    "    for c in pool:\n",
    "        if (c[\"step_class\"] == \"step\"\n",
    "            and c.get(\"step_num\") and int(c[\"step_num\"]) == tgt - 1\n",
    "            and same_dataset(c, q)):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# FOR DEBUG PURPOSES DO NOT USE IN INFERENCE\n",
    "def fallback_system_msg(rec, prev_caption):\n",
    "    mid  = manual_id(rec[\"image_path\"])\n",
    "    step = rec.get(\"step_num\",\"unknown\")\n",
    "    if rec[\"task\"] == \"grounding\":\n",
    "        line =(f\"(Manual {mid}, step {step}). \"\n",
    "               f\"{'Previous instruction: “'+prev_caption+'”. ' if prev_caption else ''}\"\n",
    "               \"Describe concisely the next building action; answer even if uncertain.\")\n",
    "    elif rec[\"task\"] == \"object\":\n",
    "        line = f\"(Manual {mid}, eop step {step}).\"\n",
    "    else:\n",
    "        line = f\"(Manual {mid}, step {step}).\"\n",
    "    return {\"role\":\"system\",\"content\":line}\n",
    "\n",
    "def ok_candidate(task, rec):\n",
    "    sc, txt = rec.get(\"step_class\"), rec.get(\"text\", \"\")\n",
    "    return ((task == \"grounding\" and sc == \"step\") or\n",
    "            (task == \"object\"    and sc == \"eop\") or\n",
    "            (task == \"state\"     and sc == \"step\"))\n",
    "\n",
    "by_manual_full = defaultdict(list)\n",
    "for r in ds.data:\n",
    "    if r[\"task\"] == \"object\" and r.get(\"step_class\") == \"eop\":\n",
    "        by_manual_full[manual_id(r[\"image_path\"])].append(r)\n",
    "\n",
    "by_manual_state_eop_full = defaultdict(list)\n",
    "for r in ds.data:\n",
    "    if r[\"task\"] == \"state\" and r.get(\"step_class\") == \"eop\":\n",
    "        by_manual_state_eop_full[manual_id(r[\"image_path\"])].append(r)\n",
    "by_manual_state_full = defaultdict(list)\n",
    "for r in ds.data:\n",
    "    if r[\"task\"] == \"state\" and r.get(\"step_class\") == \"step\":\n",
    "        by_manual_state_full[manual_id(r[\"image_path\"])].append(r)\n",
    "\n",
    "\n",
    "OBJ_RE = re.compile(r\"<p>[^<]+</p>\")\n",
    "def count_objs(resp_text: str) -> int:\n",
    "    return len(OBJ_RE.findall(resp_text or \"\"))\n",
    "\n",
    "def pick_demo(rec):\n",
    "    task = rec[\"task\"]\n",
    "    manual = manual_id(rec[\"image_path\"])\n",
    "\n",
    "    if task == \"grounding\":\n",
    "        # unchanged\n",
    "        return immediate_prev_step(rec, by_manual.get(manual, [])), None\n",
    "\n",
    "    if task == \"object\":\n",
    "        # count how many objects the query expects\n",
    "        gt_n = count_objs(rec[\"response\"])\n",
    "        # only pick demos from dataset in same manual & same count\n",
    "        cand = [\n",
    "            r for r in by_manual_full[manual]\n",
    "            if r is not rec\n",
    "            and count_objs(r[\"response\"]) == gt_n\n",
    "        ]\n",
    "        demo = random.choice(cand) if cand else None\n",
    "        return demo, (\"success\" if demo else \"none\")\n",
    "\n",
    "    \n",
    "    if task == \"state\":\n",
    "        # build full manual‐level state pool\n",
    "        pool = [r for r in ds.data\n",
    "                if r[\"task\"]==\"state\"\n",
    "                    and manual_id(r[\"image_path\"])==manual\n",
    "                    and r is not rec]\n",
    "    \n",
    "        real_cands = [r for r in pool if \"_fake\" not in r[\"image_path\"]]\n",
    "        fake_cands = [r for r in pool if \"_fake\" in r[\"image_path\"]]\n",
    "    \n",
    "    \n",
    "        if real_cands and fake_cands:\n",
    "            demo_yes = random.choice(real_cands)\n",
    "            demo_no = None\n",
    "            stage = \"\"\n",
    "    \n",
    "            real_path = Path(demo_yes['image_path'])\n",
    "            real_base_name = real_path.name.replace(\"\".join(real_path.suffixes), \"\")\n",
    "    \n",
    "            related_fakes = [\n",
    "                r for r in fake_cands\n",
    "                if Path(r['image_path']).name.startswith(f\"{real_base_name}_fake\")\n",
    "            ]\n",
    "    \n",
    "            if related_fakes:\n",
    "                demo_no = random.choice(related_fakes)\n",
    "                stage = \"pair (related)\"\n",
    "            else:\n",
    "                demo_no = random.choice(fake_cands)\n",
    "                stage = \"pair (unrelated)\"\n",
    "\n",
    "            return [demo_yes, demo_no], stage\n",
    "        else:\n",
    "            # No change to the case where candidates are missing\n",
    "            print(f\"[DEBUG pick_demo] no state-pair for manual {manual}\")\n",
    "            return None, \"none\"\n",
    "\n",
    "    # fallback\n",
    "    return None, \"none\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d03825",
   "metadata": {},
   "source": [
    "### Download the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c6ff4-797c-40c0-a24f-e847a80032ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bperju/qwen-vl2.5-r1-2\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_folder = snapshot_download(\n",
    "    repo_id=\"Bperju/qwen-vl2.5-r1-2\",\n",
    "    repo_type=\"model\",\n",
    "    local_dir=\"./\",             # where to put the files\n",
    ")\n",
    "\n",
    "print(\"All files are in:\", local_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d351e",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf81987",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### UNCOMMENT TO LOAD THE BASE MODEL\n",
    "'''\n",
    "device    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 2600*28*28\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    padding_side=\"left\",\n",
    "    use_fast=True,\n",
    "    #min_pixels=min_pixels,\n",
    "    max_pixels=max_pixels,\n",
    ")\n",
    "processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "'''\n",
    "# UNCOMMENT TO LOAD THE REFCOCO TRAINED MODEL\n",
    "'''\n",
    "\n",
    "device    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "\n",
    "model_name = \"omlab/Qwen2.5VL-3B-VLM-R1-REC-500steps\"\n",
    "\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 2600*28*28\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    padding_side=\"left\",\n",
    "    use_fast=True,\n",
    "    #min_pixels=min_pixels,\n",
    "    max_pixels=max_pixels,\n",
    ")\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "'''\n",
    "\n",
    "# UNCOMMENT TO LOAD THE TRAINED MODEL\n",
    "'''\n",
    "%pip install peft\n",
    "from peft import PeftModel\n",
    "\n",
    "device    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) load base with untied embeddings\n",
    "base = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    tie_word_embeddings=False,              # ← untie to avoid warning\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 2) copy input embedding into lm_head\n",
    "base.lm_head.weight.data = base.get_input_embeddings().weight.data.clone()\n",
    "\n",
    "# 3) attach PEFT adapter\n",
    "adapter_path = \"checkpoint/checkpoint-500/\"\n",
    "model = PeftModel.from_pretrained(base, adapter_path, device_map=\"auto\")\n",
    "\n",
    "# 4) (optional) fuse for speed\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 2600*28*28\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    padding_side=\"left\",\n",
    "    use_fast=True,\n",
    "    #min_pixels=min_pixels,\n",
    "    max_pixels=max_pixels,\n",
    ")\n",
    "processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaff80a",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e8800-f67e-4bfc-adb6-9b9fe154e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_example = True\n",
    "batch_size      = 6\n",
    "device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "progress_path = Path(\"progress.json\")\n",
    "eval_handles  = {t: Path(f\"eval_{t}.jsonl\").open(\"a\", encoding=\"utf-8\")\n",
    "                 for t in fmt}\n",
    "\n",
    "#temp if you only want to run inference for the object task\n",
    "#records = [r for r in records if r[\"task\"] == \"object\"]\n",
    "stats, total, processed = {t: defaultdict(list) for t in fmt}, len(records), 0\n",
    "if hasattr(processor.image_processor, \"antialias\"):\n",
    "    processor.image_processor.antialias = True\n",
    "for start in tqdm(range(0, total, batch_size), desc=\"Batches\"):\n",
    "    batch = records[start:start+batch_size]\n",
    "    prompts, img_batches = [], []\n",
    "\n",
    "    for rec in batch:\n",
    "        task, msgs = rec[\"task\"], []\n",
    "        demo, stage = pick_demo(rec)\n",
    "        prev_caption = rec.get(\"prev_instruction\", \"\").strip()\n",
    "        \n",
    "        if include_example and task == \"state\" and isinstance(demo, list):\n",
    "            for d in demo:\n",
    "                demo_gt = \"Yes.\" if \"_fake\" not in d[\"image_path\"] else \"No.\"\n",
    "        \n",
    "                msgs.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": f\"file://{d['image_path']}\"},\n",
    "                        {\"type\": \"text\",  \"text\": add_fmt(d[\"prompt\"], task)}\n",
    "                    ]\n",
    "                })\n",
    "                msgs.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": demo_gt}] \n",
    "                })\n",
    "    \n",
    "        elif demo and include_example:\n",
    "            msgs.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": f\"file://{demo['image_path']}\"},\n",
    "                    {\"type\": \"text\",  \"text\": add_fmt(demo[\"prompt\"], task)}\n",
    "                ]\n",
    "            })\n",
    "            msgs.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": demo[\"response\"]}]\n",
    "            })\n",
    "\n",
    "        if task in (\"grounding\", \"state\") and prev_caption:\n",
    "            msgs.append({\n",
    "                \"role\":    \"system\",\n",
    "                \"content\": f\"Previous instruction: {prev_caption}\"\n",
    "            })\n",
    "        elif demo is None:\n",
    "            msgs.append(fallback_system_msg(rec, prev_caption))\n",
    "\n",
    "        # Remove \"{Question}...\" if running non reasoning model\n",
    "        q_txt = add_fmt(rec[\"prompt\"], task)\n",
    "        msgs.append({\n",
    "            \"role\":\"user\", \"content\":[\n",
    "                {\"type\": \"image\", \"image\": f\"file://{rec['image_path']}\"},\n",
    "                {\"type\":\"text\", \"text\":f\"{{Question}} First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags.{q_txt}\"}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # bookkeeping\n",
    "        if demo is None:\n",
    "            demo_paths = None\n",
    "        elif isinstance(demo, list):\n",
    "            demo_paths = [d[\"image_path\"] for d in demo]\n",
    "        else:\n",
    "            demo_paths = demo[\"image_path\"]\n",
    "    \n",
    "        stats[task][stage].append({\n",
    "            \"query\":        rec[\"image_path\"],\n",
    "            \"demo\":         demo_paths,\n",
    "            \"prev_caption\": bool(prev_caption),\n",
    "            \"messages\":     msgs\n",
    "        })\n",
    "        rec[\"_user_messages\"] = msgs\n",
    "\n",
    "        prompts.append(processor.apply_chat_template(\n",
    "            msgs, tokenize=False, add_generation_prompt=True))\n",
    "        imgs,_ = process_vision_info(msgs)\n",
    "        img_batches.append(imgs)\n",
    "\n",
    "    # model inference\n",
    "    inputs = processor(text=prompts,\n",
    "                       images=img_batches,\n",
    "                       padding=True,\n",
    "                       truncation=False,\n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outs = model.generate(**inputs,\n",
    "                              max_new_tokens=128,\n",
    "                              use_cache=True,\n",
    "                              do_sample=False, \n",
    "                              num_beams=1\n",
    "                             )\n",
    "\n",
    "    new_tok = [o[i.shape[-1]:] for i,o in zip(inputs.input_ids, outs)]\n",
    "    preds   = processor.batch_decode(new_tok,\n",
    "                                     skip_special_tokens=True,\n",
    "                                     clean_up_tokenization_spaces=False)\n",
    "\n",
    "    for rec, pred in zip(batch, preds):\n",
    "        fh = eval_handles[rec[\"task\"]]\n",
    "\n",
    "        if rec[\"task\"] == \"state\":\n",
    "            if \"_fake\" in rec[\"image_path\"]:\n",
    "                gt = \"No.\"\n",
    "            else:\n",
    "                gt = \"Yes.\"\n",
    "            print(f\"[DEBUG write] STATE {rec['image_path']} → GT = {gt}\")\n",
    "        else:\n",
    "            gt = rec[\"response\"]\n",
    "\n",
    "        fh.write(json.dumps({\n",
    "            \"messages\":     rec[\"_user_messages\"],\n",
    "            \"ground_truth\": gt,\n",
    "            \"prediction\":   pred\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "        fh.flush(); os.fsync(fh.fileno())\n",
    "        processed += 1\n",
    "\n",
    "    progress_path.write_text(json.dumps(\n",
    "        {\"processed\": f\"{processed}/{total}\",\n",
    "         \"examples\":  stats},\n",
    "        ensure_ascii=False, indent=2))\n",
    "\n",
    "    # cleanup \n",
    "    del inputs, outs, new_tok, preds, img_batches\n",
    "    torch.cuda.empty_cache(); torch.cuda.ipc_collect(); gc.collect()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
